# Qwen3 Fine-tuning Practice: Medical R1 Reasoning Style Chat

[ä¸­æ–‡](README.md)

[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/overview)

- **Base Model**: [Qwen3-0.6B](https://modelscope.cn/models/Qwen/Qwen3-0.6B/summary)
- **Fine-tuned Model**: [Qwen3-1.7b-Medical-R1-sft](https://modelscope.cn/models/testUser/Qwen3-1.7b-Medical-R1-sft/summary)
- **Dataset**: [delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)
- **SwanLab**: [qwen3-sft-medical](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)
- **Fine-tuning Methods**: Full Parameter Fine-tuning, LoRA Fine-tuning
- **Reasoning Style**: R1 Reasoning Style
- **Computing Requirements**:
  - **Full Parameter Fine-tuning**: 8GB GPU Memory
  - **LoRA Fine-tuning**: 6GB GPU Memory

> To reduce GPU memory requirements, the Qwen3-0.6B model is used.

## ğŸ“Š Model Evaluation Results

### Quantitative Evaluation Comparison

#### Full Parameter Fine-tuning vs Base Model
| Evaluation Metric | Base Model | Full Parameter Fine-tuned Model | Improvement |
|---------|---------|--------------|---------|
| **Perplexity** | 1.8568 | 1.8441 | **Reduced by 0.0127** |
| **BLEU Score** | 0.1154 | 0.1869 | **Improved by 61.87%** |
| **ROUGE-1 F1** | 0.0563 | 0.0952 | **Improved by 68.92%** |
| **Vocabulary F1** | 0.4487 | 0.5427 | **Improved by 20.94%** |
| **Precision** | 0.4921 | 0.5922 | **Improved by 20.33%** |
| **Recall** | 0.4174 | 0.5094 | **Improved by 22.04%** |

#### LoRA Fine-tuning vs Base Model
| Evaluation Metric | Base Model | LoRA Fine-tuned Model | Improvement |
|---------|---------|-------------|---------|
| **Perplexity** | 1.9060 | 2.0242 | Increased by 6.20% |
| **BLEU Score** | 0.1235 | 0.1539 | **Improved by 24.63%** |
| **ROUGE-1 F1** | 0.0538 | 0.0847 | **Improved by 57.43%** |
| **Vocabulary F1** | 0.4470 | 0.4902 | **Improved by 9.67%** |
| **Precision** | 0.4995 | 0.5940 | **Improved by 18.92%** |
| **Recall** | 0.4091 | 0.4314 | **Improved by 5.45%** |

> **Key Finding**: Full parameter fine-tuning outperforms LoRA fine-tuning in almost all evaluation metrics, with significant improvements in text generation quality (BLEU, ROUGE).

### Qualitative Evaluation Examples

#### Example 1: Role of Endoscopic Ultrasound in Gastric Cancer Diagnosis
**Question**: Doctor, I heard that endoscopic ultrasound is very helpful in diagnosing gastric cancer. What specific role does it play?

**Base Model Answer**:
- Contains thinking process: Thought about the basic functions of endoscopic ultrasound, but some descriptions are not accurate
- Answer structure: Lists 4 functions including enhancing diagnostic accuracy, localization and biopsy
- Accuracy: Some descriptions have confusion with medical concepts

**Full Parameter Fine-tuned Model Answer**:
- Thinking process: More professional medical reasoning, considers specific applications of endoscopic ultrasound in gastric cancer staging
- Answer focus: Accurately describes key roles of endoscopic ultrasound in judging tumor invasion depth, evaluating lymph node metastasis
- Professionalism: Uses correct medical terminology, explanations are more consistent with clinical practice

#### Example 2: Examination Methods for Bronchiectasis
**Question**: Doctor, I've been feeling difficulty breathing recently, sometimes coughing up yellow phlegm. I'd like to know what examination methods are available for bronchiectasis?

**Base Model Answer**:
- Misdiagnosed bronchiectasis as bronchial asthma
- Inaccurate listing of examination methods, includes irrelevant ECG examination
- Lacks detailed explanation of key diagnostic methods like sputum examination

**Full Parameter Fine-tuned Model Answer**:
- Correctly identified bronchiectasis and related symptoms
- Comprehensively listed correct methods including imaging examinations (chest X-ray, high-resolution CT), laboratory tests, microbiological examinations
- Detailed explanation of the clinical significance and diagnostic value of each examination

## Project Structure

```
qwenMedSft/
â”œâ”€â”€ dataset/                    # Dataset directory
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â””â”€â”€ val.jsonl
â”œâ”€â”€ output/                    # Trained model output
â”‚   â”œâ”€â”€ Qwen3-0.6B/            # LoRA fine-tuned model
â”‚   â”‚   â”œâ”€â”€ checkpoint-400
â”‚   â”‚   â”œâ”€â”€ checkpoint-800
â”‚   â”‚   â””â”€â”€ checkpoint-1082
â”‚   â”œâ”€â”€ Qwen3-param/           # Full parameter fine-tuned model
â”‚   â”‚   â”œâ”€â”€ checkpoint-400
â”‚   â”‚   â”œâ”€â”€ checkpoint-800
â”‚   â”‚   â””â”€â”€ checkpoint-1082
â”œâ”€â”€ Qwen3-0.6B/               # Base model
â”œâ”€â”€ results/                  # Experiment result files
â”‚   â”œâ”€â”€ model_comparison_results.json          # LoRA fine-tuning comparison results
â”‚   â””â”€â”€ full_param_model_comparison_results.json  # Full parameter fine-tuning comparison results
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ data.py               # Data loading and splitting
â”‚   â”œâ”€â”€ train_lora.py         # LoRA fine-tuning training
â”‚   â”œâ”€â”€ train.py              # Full parameter fine-tuning training
â”‚   â”œâ”€â”€ model_compare.py      # LoRA model comparison evaluation
â”‚   â”œâ”€â”€ compare_param.py      # Full parameter model comparison evaluation
â”‚   â””â”€â”€ infer.py              # Three-model comparison inference
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run.sh                # Experiment running script
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Project documentation
```

## Environment Installation

```bash
pip install -r requirements.txt
```

## Data Preparation

Automatically completes dataset download, preprocessing, validation set splitting, generating `train.jsonl` and `val.jsonl` files:

```bash
python src/data.py
```

## Training

### Full Parameter Fine-tuning
```bash
python src/train.py
```

### LoRA Fine-tuning
```bash
python src/train_lora.py
```

### SwanLab Training Logs
[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/overview)

Comparison of two fine-tuning methods. Tests show full parameter fine-tuning performs better than LoRA:

![](./readme_images/charts.png)

## Model Evaluation

### LoRA Fine-tuned Model Evaluation
```bash
python src/model_compare.py
```
Results saved to: `results/model_comparison_results.json`

### Full Parameter Fine-tuned Model Evaluation
```bash
python src/compare_param.py
```
Results saved to: `results/full_param_model_comparison_results.json`

### Evaluation Metrics Explanation
1. **Perplexity**: Measures model prediction capability, lower is better
2. **BLEU Score**: Evaluates similarity between generated text and reference text
3. **ROUGE Score**: Measures recall-oriented text similarity
4. **F1/Precision/Recall**: Vocabulary overlap-based text matching evaluation

## Inference Testing

### Three-Model Comparison Inference
```bash
python src/infer.py
```

### LoRA Fine-tuned Model Single Inference
```bash
python src/test_lora.py
```

### Full Parameter Fine-tuned Model Single Inference
```bash
python src/test_param.py
```

## Training Effect Example

**Question**: Doctor, I've been having stomach discomfort recently. I heard there are several anti-ulcer drugs for treatment. Could youè¯¦ç»†ä»‹ç»è¿™äº›è¯ç‰©çš„åˆ†ç±»ã€ä½œç”¨æœºåˆ¶ä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•å½±å“èƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡çš„ï¼Ÿ

**Model Answer (with thinking process)**:
```xml
<think>
ç”¨æˆ·é—®çš„æ˜¯æŠ—æºƒç–¡è¯ç‰©çš„åˆ†ç±»ã€ä½œç”¨æœºåˆ¶ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å½±å“èƒƒé»è†œçš„ä¿æŠ¤å’ŒæŸä¼¤å¹³è¡¡ã€‚
é¦–å…ˆï¼Œæˆ‘éœ€è¦å›å¿†æŠ—æºƒç–¡è¯ç‰©çš„ä¸»è¦åˆ†ç±»ã€‚ä¸»è¦åˆ†ä¸ºå‡ å¤§ç±»ï¼šæŠ‘é…¸è¯ã€èƒƒé»è†œä¿æŠ¤å‰‚ã€ä¿ƒèƒƒåŠ¨åŠ›è¯å’ŒæŠ—å¹½é—¨èºæ†èŒè¯ç‰©ã€‚
æŠ‘é…¸è¯åŒ…æ‹¬H2å—ä½“æ‹®æŠ—å‰‚å’Œè´¨å­æ³µæŠ‘åˆ¶å‰‚ï¼Œä½œç”¨æœºåˆ¶æ˜¯æŠ‘åˆ¶èƒƒé…¸åˆ†æ³Œã€‚èƒƒé»è†œä¿æŠ¤å‰‚å¦‚ç¡«ç³–é“ã€é“‹å‰‚ï¼Œé€šè¿‡å½¢æˆä¿æŠ¤å±‚æˆ–ä¿ƒè¿›é»æ¶²åˆ†æ³Œæ¥ä¿æŠ¤é»è†œã€‚ä¿ƒèƒƒåŠ¨åŠ›è¯é€šè¿‡å¢å¼ºèƒƒè •åŠ¨å‡å°‘åæµã€‚æŠ—å¹½é—¨èºæ†èŒè¯ç‰©é€šè¿‡æŠ‘åˆ¶ç»†èŒç”Ÿé•¿å’Œå‡å°‘èƒƒé…¸åˆ†æ³Œæ¥æ²»ç–—æºƒç–¡ã€‚
è¿™äº›è¯ç‰©é€šè¿‡ä¸åŒæœºåˆ¶å¹³è¡¡èƒƒé»è†œçš„ä¿æŠ¤å’ŒæŸä¼¤ï¼šæŠ‘é…¸è¯å‡å°‘æŸä¼¤å› ç´ ï¼Œä¿æŠ¤å‰‚å¢å¼ºé˜²å¾¡ï¼Œä¿ƒèƒƒåŠ¨åŠ›è¯å‡å°‘åæµæŸä¼¤ï¼ŒæŠ—å¹½é—¨èºæ†èŒè¯ç‰©æ¶ˆé™¤ç—…åŸä½“ã€‚
</think>

å½“ç„¶å¯ä»¥ã€‚æŠ—æºƒç–¡è¯ç‰©ä¸»è¦åˆ†ä¸ºå››ç±»ï¼šæŠ‘é…¸è¯ã€èƒƒé»è†œä¿æŠ¤å‰‚ã€ä¿ƒèƒƒåŠ¨åŠ›è¯å’ŒæŠ—å¹½é—¨èºæ†èŒè¯ç‰©ã€‚
1. **æŠ‘é…¸è¯**ï¼šåŒ…æ‹¬Hâ‚‚å—ä½“æ‹®æŠ—å‰‚å’Œè´¨å­æ³µæŠ‘åˆ¶å‰‚ï¼Œé€šè¿‡æŠ‘åˆ¶èƒƒé…¸åˆ†æ³Œå‡å°‘å¯¹èƒƒé»è†œçš„åŒ–å­¦æŸä¼¤ã€‚
2. **èƒƒé»è†œä¿æŠ¤å‰‚**ï¼šå¦‚ç¡«ç³–é“å’Œèƒ¶ä½“æœèƒ¶é“‹ï¼Œåœ¨èƒƒé»è†œè¡¨é¢å½¢æˆä¿æŠ¤è†œï¼Œéš”ç¦»èƒƒé…¸å’Œèƒƒè›‹ç™½é…¶çš„ä¾µèš€ã€‚
3. **ä¿ƒèƒƒåŠ¨åŠ›è¯**ï¼šå¦‚å¤šæ½˜ç«‹é…®ï¼Œå¢å¼ºèƒƒæ’ç©ºï¼Œå‡å°‘èƒƒå†…é£Ÿç‰©å’Œèƒƒé…¸çš„åæµæ—¶é—´ã€‚
4. **æŠ—å¹½é—¨èºæ†èŒè¯ç‰©**ï¼šé€šå¸¸é‡‡ç”¨æŠ—ç”Ÿç´ è”åˆè´¨å­æ³µæŠ‘åˆ¶å‰‚ï¼Œæ ¹é™¤å¹½é—¨èºæ†èŒæ„ŸæŸ“ï¼Œæ¶ˆé™¤ä¸»è¦ç—…å› ã€‚
è¿™äº›è¯ç‰©å…±åŒä½œç”¨ï¼Œé€šè¿‡å‡å°‘æ”»å‡»å› ç´ ï¼ˆèƒƒé…¸ã€å¹½é—¨èºæ†èŒï¼‰å’Œå¢å¼ºé˜²å¾¡å› ç´ ï¼ˆé»è†œå±éšœã€èƒƒæ’ç©ºï¼‰æ¥æ¢å¤èƒƒé»è†œçš„ä¿æŠ¤ä¸æŸä¼¤å¹³è¡¡ã€‚
```

## Technical Points

### R1 Reasoning Style
- Model generates `<think>` thinking process before answering
- Thinking process demonstrates model's reasoning logic and knowledge retrieval process
- Final answer is based on summary and refinement of the thinking process

### Fine-tuning Strategy Comparison
1. **Full Parameter Fine-tuning**:
   - Updates all model parameters
   - Higher training cost but better results
   - More suitable for professional domain knowledge learning

2. **LoRA Fine-tuning**:
   - Only trains low-rank adapters
   - High training efficiency, low memory requirements
   - Suitable for resource-constrained scenarios

### Evaluation Methods
- Quantitative evaluation using 50 random samples
- Qualitative analysis including 5 detailed samples
- Comprehensive multiple NLP evaluation metrics

## Related Tools

- [swanlab](https://github.com/SwanHubX/SwanLab): Open-source, modern-designed deep learning training tracking and visualization tool
- [transformers](https://github.com/huggingface/transformers): HuggingFace's library containing pre-trained models for text, computer vision, audio, video and multimodal tasks, for inference and training
- [peft](https://github.com/huggingface/peft): Library for efficient fine-tuning of large language models

## Conclusion

This project demonstrates the effectiveness of fine-tuning the Qwen3 model in the medical domain. By comparing full parameter fine-tuning and LoRA fine-tuning methods, we found:
1. Full parameter fine-tuning performs better in professional medical Q&A
2. R1 reasoning style effectively improves answer professionalism and explainability
3. Even with the limited 0.6B parameter scale, the model can still learn professional medical knowledge

Future work could explore fine-tuning larger parameter-scale models and training with more diverse medical datasets.